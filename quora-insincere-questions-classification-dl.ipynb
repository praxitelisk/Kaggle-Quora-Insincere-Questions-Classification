{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "df_test = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "17b20f13609657a4668159762c0e7d81b2d558c0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00004f9a462a357c33be</td>\n",
       "      <td>Is Gaza slowly becoming Auschwitz, Dachau or T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00005059a06ee19e11ad</td>\n",
       "      <td>Why does Quora automatically ban conservative ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0000559f875832745e2e</td>\n",
       "      <td>Is it crazy if I wash or wipe my groceries off...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00005bd3426b2d0c8305</td>\n",
       "      <td>Is there such a thing as dressing moderately, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00006e6928c5df60eacb</td>\n",
       "      <td>Is it just me or have you ever been in this ph...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "3  000042bf85aa498cd78e  ...        0\n",
       "4  0000455dfa3e01eae3af  ...        0\n",
       "5  00004f9a462a357c33be  ...        0\n",
       "6  00005059a06ee19e11ad  ...        0\n",
       "7  0000559f875832745e2e  ...        0\n",
       "8  00005bd3426b2d0c8305  ...        0\n",
       "9  00006e6928c5df60eacb  ...        0\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "59ea39d66cf8a545d0596ed186ab6453906e5446"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid              0\n",
       "question_text    0\n",
       "target           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "98ec366ffbddcc8907afd9cd0b451a84863230ef"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00014894849d00ba98a9</td>\n",
       "      <td>My voice range is A2-C5. My chest voice goes u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000156468431f09b3cae</td>\n",
       "      <td>How much does a tutor earn in Bangalore?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000227734433360e1aae</td>\n",
       "      <td>What are the best made pocket knives under $20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0005e06fbe3045bd2a92</td>\n",
       "      <td>Why would they add a hypothetical scenario tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00068a0f7f41f50fc399</td>\n",
       "      <td>What is the dresscode for Techmahindra freshers?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>000a2d30e3ffd70c070d</td>\n",
       "      <td>How well are you adapting to the Trump era?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>000b67672ec9622ff761</td>\n",
       "      <td>What should be the last thing people do in life?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000b7fb1146d712c1105</td>\n",
       "      <td>Received conditional offer for Masters in Inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>000d665a8ddc426a1907</td>\n",
       "      <td>What does appareils photo mean in French?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>000df6fd2229447b2969</td>\n",
       "      <td>Is there a system of Public Interest Litigatio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text\n",
       "0  00014894849d00ba98a9  My voice range is A2-C5. My chest voice goes u...\n",
       "1  000156468431f09b3cae           How much does a tutor earn in Bangalore?\n",
       "2  000227734433360e1aae  What are the best made pocket knives under $20...\n",
       "3  0005e06fbe3045bd2a92  Why would they add a hypothetical scenario tha...\n",
       "4  00068a0f7f41f50fc399   What is the dresscode for Techmahindra freshers?\n",
       "5  000a2d30e3ffd70c070d        How well are you adapting to the Trump era?\n",
       "6  000b67672ec9622ff761   What should be the last thing people do in life?\n",
       "7  000b7fb1146d712c1105  Received conditional offer for Masters in Inte...\n",
       "8  000d665a8ddc426a1907          What does appareils photo mean in French?\n",
       "9  000df6fd2229447b2969  Is there a system of Public Interest Litigatio..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "a3d0a7d37cd150535a5a5d587cfcbf75d0205956"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "qid              0\n",
       "question_text    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "e2a1d8254c84a2a67930f9a54b0057490c6d4d4c"
   },
   "outputs": [],
   "source": [
    "def tokenize_the_text(phrases):\n",
    "    \n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from nltk.text import Text\n",
    "    \n",
    "    tokens = [word for word in phrases]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    tokens = [word_tokenize(word) for word in tokens]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "#crude_tokens = tokenize_the_text(df.question_text)\n",
    "#print(crude_tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "f920a342fa3e0298e67c53f6fd3b6bf86e6b6b19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 263186 words\n"
     ]
    }
   ],
   "source": [
    "def create_a_vocab(tokens):\n",
    "    \n",
    "    vocab = set()\n",
    "\n",
    "    for sentence in tokens:\n",
    "        for word in sentence:\n",
    "            vocab.add(word)\n",
    "\n",
    "    vocab = list(vocab)\n",
    "\n",
    "    return vocab\n",
    "    \n",
    "#vocab = create_a_vocab(crude_tokens)\n",
    "\n",
    "vocab = create_a_vocab(tokenize_the_text(df.question_text))\n",
    "\n",
    "print(\"Vocabulary size:\", len(vocab), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "3e9ff461ef015e09a4a5f534c56de024501ba0a7"
   },
   "outputs": [],
   "source": [
    "def removing_stopwords(tokens_custom_cleaned):\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    tokens_custom_cleaned_and_without_stopwords = []\n",
    "    for sentence in tokens_custom_cleaned:\n",
    "        tokens_custom_cleaned_and_without_stopwords.append([word for word in sentence if word not in stop_words])\n",
    "        \n",
    "    return tokens_custom_cleaned_and_without_stopwords\n",
    "\n",
    "tokens_without_stopwords = removing_stopwords(tokenize_the_text(df.question_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "2a4ebb3e12a34dfc4e5f3d1470b4384d7c46dc79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size after removing stopwords: 263033 words\n"
     ]
    }
   ],
   "source": [
    "vocab = create_a_vocab(tokens_without_stopwords)\n",
    "\n",
    "print(\"Vocabulary size after removing stopwords:\", len(vocab), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "565065f95e2f7c3700ab13f53d463226b54d61ce"
   },
   "outputs": [],
   "source": [
    "def lemmatizing_the_tokens(tokens_custom_cleaned_and_without_stopwords):\n",
    "\n",
    "    from nltk.stem.wordnet import WordNetLemmatizer \n",
    "    lem = WordNetLemmatizer()\n",
    "\n",
    "    tokens_custom_cleaned_and_without_stopwords_and_lemmatized = []\n",
    "\n",
    "    for sentence in tokens_custom_cleaned_and_without_stopwords:\n",
    "        tokens_custom_cleaned_and_without_stopwords_and_lemmatized.append([lem.lemmatize(word, pos='v') for word in sentence])\n",
    "        \n",
    "    return tokens_custom_cleaned_and_without_stopwords_and_lemmatized\n",
    "\n",
    "\n",
    "tokens_without_stopwords_and_lemmatized = lemmatizing_the_tokens(tokens_without_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "0d1775be8fa242ec1e55d1232b118f56c90145cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size after removing stopwords and lemmatizing the text: 250313 words\n"
     ]
    }
   ],
   "source": [
    "vocab = create_a_vocab(tokens_without_stopwords_and_lemmatized)\n",
    "\n",
    "print(\"Vocabulary size after removing stopwords and lemmatizing the text:\", len(vocab), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "fe34fb76097ee68a544d17346d8d752b19d18c16"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size after removing stopwords and lemmatizing the text: 37597 words\n"
     ]
    }
   ],
   "source": [
    "### do the same and for the testSet\n",
    "\n",
    "\n",
    "tokens_without_stopwords_test = removing_stopwords(tokenize_the_text(df_test.question_text))\n",
    "tokens_without_stopwords_and_lemmatized_test = lemmatizing_the_tokens(tokens_without_stopwords_test)\n",
    "\n",
    "vocab_test = create_a_vocab(tokens_without_stopwords_and_lemmatized_test)\n",
    "\n",
    "print(\"Vocabulary size after removing stopwords and lemmatizing the text:\", len(vocab_test), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "909387b13deca492e5b7e4583e438f4f08cd8b9c"
   },
   "outputs": [],
   "source": [
    "del tokens_without_stopwords_test\n",
    "del tokens_without_stopwords\n",
    "del vocab\n",
    "del vocab_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "98f4a00bb4d49b7f043d6d7e09bcab03ed56072f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Keras Libraries\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, GRU, Conv1D, MaxPooling1D, Dropout, SpatialDropout1D, Bidirectional, Activation,GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "07bf7060e60f3258db9ae6133759554973203697"
   },
   "outputs": [],
   "source": [
    "def get_embeddings_dict():\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    filename = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "\n",
    "    glove_w2v_embeddings_index = dict()\n",
    "    f = open(filename, \"r\", encoding='utf-8')\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        glove_w2v_embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    return glove_w2v_embeddings_index\n",
    "\n",
    "\n",
    "glove_w2v_embeddings_index = get_embeddings_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "35d1d4a262ea2c29fca16fe15dd15cabd0638c35"
   },
   "outputs": [],
   "source": [
    "###\n",
    "#tokens_uncleaned = tokenize_the_text(df.question_text.values)\n",
    "#tokens_uncleaned_test = tokenize_the_text(df_test.question_text.values)\n",
    "#vocab_all_corpus = create_a_vocab(tokenize_the_text(df.question_text.values) + tokenize_the_text(df_test.question_text.values))\n",
    "\n",
    "#sentences = [' '.join(sent) for sent in tokens_uncleaned]\n",
    "#sentences_test = [' '.join(sent) for sent in tokens_uncleaned_test]\n",
    "\n",
    "\n",
    "all_corpus = [' '.join(sent) for sent in tokens_without_stopwords_and_lemmatized] + [' '.join(sent) for sent in tokens_without_stopwords_and_lemmatized_test]\n",
    "max_len = max([len(elem.split()) for elem in all_corpus])\n",
    "#print(max_len)\n",
    "###\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(lower=True, filters='')\n",
    "tokenizer.fit_on_texts(all_corpus)\n",
    "\n",
    "vocabulary_size = len(tokenizer.word_index) + 1\n",
    "#print(vocabulary_size)\n",
    "\n",
    "\n",
    "\n",
    "X = tokenizer.texts_to_sequences([' '.join(sent) for sent in tokens_without_stopwords_and_lemmatized])\n",
    "y = df.target.values\n",
    "X = pad_sequences(X, maxlen=max_len)\n",
    "\n",
    "\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, stratify=y, random_state=42, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "e1b12ef8205a04a43b941599495656fb6dc39f6c"
   },
   "outputs": [],
   "source": [
    "del X\n",
    "del y\n",
    "del tokens_without_stopwords_and_lemmatized\n",
    "del all_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "20682234f5ff44ef1f2487eb0da9012c19fb0bf7"
   },
   "outputs": [],
   "source": [
    "import textblob\n",
    "\n",
    "embedding_dim = 300\n",
    "\n",
    "def get_embedding_matrix():\n",
    "    \n",
    "    embedding_matrix = np.zeros((vocabulary_size, embedding_dim + 2))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        if i > vocabulary_size:\n",
    "            continue\n",
    "        embedding_vector = glove_w2v_embeddings_index.get(word)\n",
    "        word_sentiment = textblob.TextBlob(word).sentiment\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = np.append(embedding_vector, [word_sentiment.polarity, word_sentiment.subjectivity])\n",
    "        else:\n",
    "            embedding_matrix[i, -2:] = [word_sentiment.polarity, word_sentiment.subjectivity]\n",
    "            \n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "3d70cbfd77534b17df69ed1ec69ac151d59f5a64"
   },
   "outputs": [],
   "source": [
    "del glove_w2v_embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "ee94c08d7799ff853bf6f7bc5f50ebade612b4e4"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    acc = history.history['categorical_accuracy']\n",
    "    val_acc = history.history['val_categorical_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Training acc')\n",
    "    plt.plot(x, val_acc, 'r', label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Training loss')\n",
    "    plt.plot(x, val_loss, 'r', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51e54d5cef365ec09007259ecc0526a5cbb524e7"
   },
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "cecd479ed42136e14f4268f4280acc44f965181e"
   },
   "outputs": [],
   "source": [
    "def build_dl_lstm_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs, filename):\n",
    "\n",
    "    ## Network architecture\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras.layers import Masking\n",
    "    from keras.initializers import Constant\n",
    "    import time\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    from numpy.random import seed\n",
    "    seed(42)\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(42)\n",
    "\n",
    "    embedding_size= embedding_dim + 2\n",
    "    batch_size = 512\n",
    "    dropouts = 0.2\n",
    "    epochs = num_of_epochs\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim = vocabulary_size, output_dim = embedding_size, input_length = max_len, \n",
    "                        weights=[embedding_matrix], trainable = False, mask_zero=True))\n",
    "    \n",
    "    model.add(SpatialDropout1D(dropouts))\n",
    "    \n",
    "    model.add(LSTM(int((embedding_size/2)-50), recurrent_dropout=dropouts, dropout=dropouts, return_sequences=False))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'nadam', metrics=['categorical_accuracy']) # RMSprop\n",
    "\n",
    "    '''\n",
    "    saves the model weights after each epoch if the val_acc loss decreased\n",
    "    '''\n",
    "    checkpointer = ModelCheckpoint(monitor='val_categorical_accuracy', mode='max', filepath=''+filename+'.hdf5', verbose=2, save_best_only=True)\n",
    "    earlyStopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=1, verbose=0, mode='max')\n",
    "\n",
    "    history = model.fit(x = xtrain, y = to_categorical(ytrain), validation_data=(xvalid, to_categorical(yvalid)), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[checkpointer, earlyStopping])\n",
    "\n",
    "    model = load_model(''+filename+'.hdf5')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return model, history, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "129dab5c689c5a04aa22b6bd1138c46ebaa1c68b"
   },
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "85b281b8bf6f62bb94785480d76d5ff3b162779f"
   },
   "outputs": [],
   "source": [
    "def build_dl_lstm_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs, filename):\n",
    "\n",
    "    ## Network architecture\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras.layers import Masking\n",
    "    from keras.initializers import Constant\n",
    "    import time\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    from numpy.random import seed\n",
    "    seed(42)\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(42)\n",
    "\n",
    "    embedding_size= embedding_dim + 2\n",
    "    batch_size = 512\n",
    "    dropouts = 0.2\n",
    "    epochs = num_of_epochs\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim = vocabulary_size, output_dim = embedding_size, input_length = max_len, \n",
    "                        weights=[embedding_matrix], trainable = False, mask_zero=True))\n",
    "    \n",
    "    model.add(SpatialDropout1D(dropouts))\n",
    "    \n",
    "    model.add(GRU(int((embedding_size/2)-50), recurrent_dropout=dropouts, dropout=dropouts, return_sequences=False))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'nadam', metrics=['categorical_accuracy']) # RMSprop\n",
    "\n",
    "    '''\n",
    "    saves the model weights after each epoch if the val_acc loss decreased\n",
    "    '''\n",
    "    checkpointer = ModelCheckpoint(monitor='val_categorical_accuracy', mode='max', filepath=''+filename+'.hdf5', verbose=2, save_best_only=True)\n",
    "    earlyStopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=1, verbose=0, mode='max')\n",
    "\n",
    "    history = model.fit(x = xtrain, y = to_categorical(ytrain), validation_data=(xvalid, to_categorical(yvalid)), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[checkpointer, earlyStopping])\n",
    "\n",
    "    model = load_model(''+filename+'.hdf5')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return model, history, elapsed_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e08e1fdd561e3085413051307501406ea362cd04"
   },
   "source": [
    "### LSTM + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "cfb0c742e9235322d308384f39e1fcc270d5c45b"
   },
   "outputs": [],
   "source": [
    "def build_dl_lstm_cnn_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs, filename):\n",
    "\n",
    "    ## Network architecture\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras.layers import Masking\n",
    "    from keras.initializers import Constant\n",
    "    import time\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    from numpy.random import seed\n",
    "    seed(42)\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(42)\n",
    "\n",
    "    embedding_size= embedding_dim + 2\n",
    "    batch_size = 1024\n",
    "    dropouts = 0.2\n",
    "    epochs = num_of_epochs\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim = vocabulary_size, output_dim = embedding_size, input_length = max_len, \n",
    "                        weights=[embedding_matrix], trainable = False))\n",
    "    \n",
    "    model.add(SpatialDropout1D(dropouts))\n",
    "    \n",
    "    model.add(GRU(units = 64, recurrent_dropout=dropouts, dropout=dropouts, return_sequences=True))\n",
    "    \n",
    "    model.add(Conv1D(128, kernel_size = 1, strides = 1,  padding='valid', activation='relu'))\n",
    "    model.add(Conv1D(256, kernel_size = 3, strides = 1,  padding='valid', activation='relu'))\n",
    "    model.add(Conv1D(512, kernel_size = 5, strides = 1,  padding='valid', activation='relu'))\n",
    "    \n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'nadam', metrics=['categorical_accuracy']) # RMSprop\n",
    "\n",
    "    '''\n",
    "    saves the model weights after each epoch if the val_acc loss decreased\n",
    "    '''\n",
    "    checkpointer = ModelCheckpoint(monitor='val_categorical_accuracy', mode='max', filepath=''+filename+'.hdf5', verbose=2, save_best_only=True)\n",
    "    earlyStopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=0, verbose=0, mode='max')\n",
    "\n",
    "    history = model.fit(x = xtrain, y = to_categorical(ytrain), validation_data=(xvalid, to_categorical(yvalid)), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[checkpointer, earlyStopping])\n",
    "\n",
    "    model = load_model(''+filename+'.hdf5')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return model, history, elapsed_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c9930700446c331436724ade8fcd77d9b57ce222"
   },
   "source": [
    "### GRU + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "af58c16c3c0f8811a07291048f00e5e8adedda7d"
   },
   "outputs": [],
   "source": [
    "def build_dl_gru_cnn_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs, filename):\n",
    "\n",
    "    ## Network architecture\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras.layers import Masking\n",
    "    from keras.initializers import Constant\n",
    "    import time\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    from numpy.random import seed\n",
    "    seed(42)\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(42)\n",
    "\n",
    "    embedding_size= embedding_dim + 2\n",
    "    batch_size = 1024\n",
    "    dropouts = 0.2\n",
    "    epochs = num_of_epochs\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim = vocabulary_size, output_dim = embedding_size, input_length = max_len, \n",
    "                        weights=[embedding_matrix], trainable = False))\n",
    "    \n",
    "    model.add(SpatialDropout1D(dropouts))\n",
    "    \n",
    "    model.add(GRU(units = 84, recurrent_dropout=dropouts, dropout=dropouts, return_sequences=True))\n",
    "    \n",
    "    model.add(Conv1D(42, kernel_size = 1, strides = 1,  padding='valid', activation='relu'))\n",
    "    #model.add(Conv1D(256, kernel_size = 3, strides = 1,  padding='valid', activation='relu'))\n",
    "    #model.add(Conv1D(512, kernel_size = 5, strides = 1,  padding='valid', activation='relu'))\n",
    "    \n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    model.add(Dense(26, activation=\"relu\"))\n",
    "    model.add(Dropout(dropouts))\n",
    "        \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'nadam', metrics=['categorical_accuracy']) # RMSprop\n",
    "\n",
    "    '''\n",
    "    saves the model weights after each epoch if the val_acc loss decreased\n",
    "    '''\n",
    "    checkpointer = ModelCheckpoint(monitor='val_categorical_accuracy', mode='max', filepath=''+filename+'.hdf5', verbose=2, save_best_only=True)\n",
    "    earlyStopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=0, verbose=0, mode='max')\n",
    "\n",
    "    history = model.fit(x = xtrain, y = to_categorical(ytrain), validation_data=(xvalid, to_categorical(yvalid)), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[checkpointer, earlyStopping])\n",
    "\n",
    "    model = load_model(''+filename+'.hdf5')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return model, history, elapsed_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6eb757aab246bf38b7ca879aabec5f38dcf77282"
   },
   "source": [
    "### Bidirectional GRU + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "39defd0e9ebc395b4fd2f4d318b0ea2b69e81d82"
   },
   "outputs": [],
   "source": [
    "def build_dl_bidirectional_gru_cnn_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs, filename):\n",
    "\n",
    "    ## Network architecture\n",
    "    from keras.utils import to_categorical\n",
    "    from keras.callbacks import ModelCheckpoint\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras.layers import Masking\n",
    "    from keras.initializers import Constant\n",
    "    import time\n",
    "    \n",
    "    import warnings\n",
    "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    from numpy.random import seed\n",
    "    seed(42)\n",
    "    from tensorflow import set_random_seed\n",
    "    set_random_seed(42)\n",
    "\n",
    "    embedding_size= embedding_dim + 2\n",
    "    batch_size = 1024\n",
    "    dropouts = 0.2\n",
    "    epochs = num_of_epochs\n",
    "    \n",
    "    model=Sequential()\n",
    "    model.add(Embedding(input_dim = vocabulary_size, output_dim = embedding_size, input_length = max_len, \n",
    "                        weights=[embedding_matrix], trainable = False))\n",
    "    \n",
    "    model.add(SpatialDropout1D(dropouts))\n",
    "    \n",
    "    model.add(Bidirectional(GRU(units = 64, recurrent_dropout=dropouts, dropout=dropouts, return_sequences=True)))\n",
    "    \n",
    "    model.add(Conv1D(128, kernel_size = 1, strides = 1,  padding='valid', activation='relu'))\n",
    "    model.add(Conv1D(256, kernel_size = 3, strides = 1,  padding='valid', activation='relu'))\n",
    "    model.add(Conv1D(512, kernel_size = 5, strides = 1,  padding='valid', activation='relu'))\n",
    "    \n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    model.add(Dense(16, activation=\"relu\"))\n",
    "    model.add(Dropout(dropouts))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer = 'nadam', metrics=['categorical_accuracy']) # RMSprop\n",
    "\n",
    "    '''\n",
    "    saves the model weights after each epoch if the val_acc loss decreased\n",
    "    '''\n",
    "    checkpointer = ModelCheckpoint(monitor='val_categorical_accuracy', mode='max', filepath=''+filename+'.hdf5', verbose=2, save_best_only=True)\n",
    "    earlyStopping = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0, patience=0, verbose=0, mode='max')\n",
    "\n",
    "    history = model.fit(x = xtrain, y = to_categorical(ytrain), validation_data=(xvalid, to_categorical(yvalid)), epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[checkpointer, earlyStopping])\n",
    "\n",
    "    model = load_model(''+filename+'.hdf5')\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    return model, history, elapsed_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "aec38f004fb92d06bde4ba7a7962853ac7f5bb95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndl_lstm_model, history_lstm, elapsed_time = build_dl_lstm_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs=30, filename=\"lstm\")\\nprint(\"Elapsed time in seconds:\", elapsed_time)\\n'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dl_lstm_model, history_lstm, elapsed_time = build_dl_lstm_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs=30, filename=\"lstm\")\n",
    "print(\"Elapsed time in seconds:\", elapsed_time)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "24f83d88588f8912195c87be81f33284f174655b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndl_lstm_cnn_model, history_lstm_cnn, elapsed_time = build_dl_lstm_cnn_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs=30, filename=\"lstm_cnn\")\\nprint(\"Elapsed time in seconds:\", elapsed_time)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dl_lstm_cnn_model, history_lstm_cnn, elapsed_time = build_dl_lstm_cnn_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs=30, filename=\"lstm_cnn\")\n",
    "print(\"Elapsed time in seconds:\", elapsed_time)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "63f16960b78dae2dd63cd31c332006467e294395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 407, 302)          77633932  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 407, 302)          0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 407, 84)           97524     \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 407, 42)           3570      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 42)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 42)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 26)                1118      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 26)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 54        \n",
      "=================================================================\n",
      "Total params: 77,736,198\n",
      "Trainable params: 102,266\n",
      "Non-trainable params: 77,633,932\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 914285 samples, validate on 391837 samples\n",
      "Epoch 1/30\n",
      " - 1015s - loss: 0.1353 - categorical_accuracy: 0.9482 - val_loss: 0.1167 - val_categorical_accuracy: 0.9551\n",
      "\n",
      "Epoch 00001: val_categorical_accuracy improved from -inf to 0.95515, saving model to gru_cnn.hdf5\n",
      "Epoch 2/30\n",
      " - 1014s - loss: 0.1199 - categorical_accuracy: 0.9533 - val_loss: 0.1108 - val_categorical_accuracy: 0.9564\n",
      "\n",
      "Epoch 00002: val_categorical_accuracy improved from 0.95515 to 0.95636, saving model to gru_cnn.hdf5\n",
      "Epoch 3/30\n",
      " - 1005s - loss: 0.1152 - categorical_accuracy: 0.9548 - val_loss: 0.1099 - val_categorical_accuracy: 0.9571\n",
      "\n",
      "Epoch 00003: val_categorical_accuracy improved from 0.95636 to 0.95714, saving model to gru_cnn.hdf5\n",
      "Epoch 4/30\n",
      " - 1004s - loss: 0.1128 - categorical_accuracy: 0.9554 - val_loss: 0.1080 - val_categorical_accuracy: 0.9567\n",
      "\n",
      "Epoch 00004: val_categorical_accuracy did not improve from 0.95714\n",
      "Elapsed time in seconds: 4045.5694041252136\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dl_gru_cnn_model, history_gru_cnn, elapsed_time = build_dl_gru_cnn_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs=30, filename=\"gru_cnn\")\n",
    "print(\"Elapsed time in seconds:\", elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "c135431e8a80542cb8f72f6f538a785596b62742"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndl_bidirectional_gru_cnn_model, history_bidirectional_gru_cnn, elapsed_time = build_dl_bidirectional_gru_cnn_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs=30, filename=\"bidirectional_gru_cnn\")\\nprint(\"Elapsed time in seconds:\", elapsed_time)\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "dl_bidirectional_gru_cnn_model, history_bidirectional_gru_cnn, elapsed_time = build_dl_bidirectional_gru_cnn_model(xtrain, ytrain, xvalid, yvalid, num_of_epochs=30, filename=\"bidirectional_gru_cnn\")\n",
    "print(\"Elapsed time in seconds:\", elapsed_time)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "4b99ed0fa007f58d1dc2bb378a5eb97f73037ffe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplot_history(history_lstm)\\n\\n\\ny_pred_lstm = dl_lstm_model.predict_classes(xvalid, verbose=1, batch_size = 256)\\nprint()\\n\\nprint(classification_report(yvalid, y_pred_lstm))\\n\\nprint()\\nprint(\"accuracy_score\", accuracy_score(yvalid, y_pred_lstm))\\n\\nprint()\\nprint(\"Weighted Averaged validation metrics\")\\nprint(\"precision_score\", precision_score(yvalid, y_pred_lstm, average=\\'weighted\\'))\\nprint(\"recall_score\", recall_score(yvalid, y_pred_lstm, average=\\'weighted\\'))\\nprint(\"f1_score\", f1_score(yvalid, y_pred_lstm, average=\\'weighted\\'))\\n\\nprint()\\nfrom sklearn.metrics import confusion_matrix\\nimport scikitplot as skplt\\nsns.set(rc={\\'figure.figsize\\':(8,8)})\\nskplt.metrics.plot_confusion_matrix(yvalid, y_pred_lstm)\\n\\n\\nprint(\"elapsed time:\", round(elapsed_time), \"seconds\")\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "plot_history(history_lstm)\n",
    "\n",
    "\n",
    "y_pred_lstm = dl_lstm_model.predict_classes(xvalid, verbose=1, batch_size = 256)\n",
    "print()\n",
    "\n",
    "print(classification_report(yvalid, y_pred_lstm))\n",
    "\n",
    "print()\n",
    "print(\"accuracy_score\", accuracy_score(yvalid, y_pred_lstm))\n",
    "\n",
    "print()\n",
    "print(\"Weighted Averaged validation metrics\")\n",
    "print(\"precision_score\", precision_score(yvalid, y_pred_lstm, average='weighted'))\n",
    "print(\"recall_score\", recall_score(yvalid, y_pred_lstm, average='weighted'))\n",
    "print(\"f1_score\", f1_score(yvalid, y_pred_lstm, average='weighted'))\n",
    "\n",
    "print()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import scikitplot as skplt\n",
    "sns.set(rc={'figure.figsize':(8,8)})\n",
    "skplt.metrics.plot_confusion_matrix(yvalid, y_pred_lstm)\n",
    "\n",
    "\n",
    "print(\"elapsed time:\", round(elapsed_time), \"seconds\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_uuid": "841cf212e242b77c07aab290cb7c5795a7c37fca"
   },
   "outputs": [],
   "source": [
    "#plot_history(history_lstm_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_uuid": "a9ab552c6d0647540a7153a18445059ed7268f55"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Sequential' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-a33c0ae6c661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_gru_cnn_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-24136c36fab4>\u001b[0m in \u001b[0;36mplot_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Sequential' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "plot_history(dl_gru_cnn_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_uuid": "38661b480682ed2f55d95c171c23ad775bd3cb18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 23s 403us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ny_pred_test_bidirectional_gru_cnn = dl_bidirectional_gru_cnn_model.predict_classes(xtest, verbose=1, batch_size = 1024)\\nsubmission = pd.DataFrame()\\nsubmission['qid'] = df_test.qid\\nsubmission['prediction'] = y_pred_test_bidirectional_gru_cnn\\n#submission['Sentiment'] = submission.Sentiment.astype(int)\\nsubmission.to_csv('submission.csv',index=False)\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest = tokenizer.texts_to_sequences([' '.join(sent) for sent in tokens_without_stopwords_and_lemmatized_test])\n",
    "xtest = pad_sequences(xtest, maxlen=max_len)\n",
    "\n",
    "'''\n",
    "y_pred_test_lstm = dl_lstm_model.predict_classes(xtest, verbose=1, batch_size = 512)\n",
    "submission = pd.DataFrame()\n",
    "submission['qid'] = df_test.qid\n",
    "submission['prediction'] = y_pred_test_lstm\n",
    "#submission['Sentiment'] = submission.Sentiment.astype(int)\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "'''\n",
    "\n",
    "'''\n",
    "y_pred_test_lstm_cnn = dl_lstm_cnn_model.predict_classes(xtest, verbose=1, batch_size = 1024)\n",
    "submission = pd.DataFrame()\n",
    "submission['qid'] = df_test.qid\n",
    "submission['prediction'] = y_pred_test_lstm_cnn\n",
    "#submission['Sentiment'] = submission.Sentiment.astype(int)\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "'''\n",
    "\n",
    "\n",
    "y_pred_test_gru_cnn = dl_gru_cnn_model.predict_classes(xtest, verbose=1, batch_size = 1024)\n",
    "submission = pd.DataFrame()\n",
    "submission['qid'] = df_test.qid\n",
    "submission['prediction'] = y_pred_test_gru_cnn\n",
    "#submission['Sentiment'] = submission.Sentiment.astype(int)\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "\n",
    "'''\n",
    "y_pred_test_bidirectional_gru_cnn = dl_bidirectional_gru_cnn_model.predict_classes(xtest, verbose=1, batch_size = 1024)\n",
    "submission = pd.DataFrame()\n",
    "submission['qid'] = df_test.qid\n",
    "submission['prediction'] = y_pred_test_bidirectional_gru_cnn\n",
    "#submission['Sentiment'] = submission.Sentiment.astype(int)\n",
    "submission.to_csv('submission.csv',index=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "_uuid": "7ff3895b026550466f74fb4a3e3466df11c312d2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
